\documentclass[11pt,english,rmp]{revtex4}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{fancyhdr}

\usepackage[dvips,pdfborderstyle={/S/U/W 1},pagebackref=true,linkcolor =blue,urlcolor=blue,bookmarks=false]{hyperref}

% Using this package causes "missing number, treated as zero ..." error
%\usepackage{parskip}
%
\usepackage[letterpaper,margin=3.5cm,mag=1414]{geometry}
%
\makeatletter
\usepackage{babel}
\makeatother
%
\setlength{\parskip}{0.2cm}
\setlength{\parindent}{6mm}

\input{../my_definitions.tex}

\begin{document}

\title{Introduction: Cosmological Considerations}


\author{Deepak Vaid}


\date{\today}

\maketitle
%\maketitle
% Activate to display a given date or no date


\tableofcontents{}


%\chapter{Cosmological Considerations}


\section{A Brief History of the Cosmological Constant Problem}

Cosmology is one of the oldest and perhaps the grandest of human sciences.
The word \emph{cosmos} is commonly understood
to represent the entire Universe. It originates from the Greek word
for order i.e. the opposite of chaos and disorder\cite{Wikipedia2008Cosmology}.
Since ancient times, across history and all civilizations the human
mind has always been drawn to the night sky and to the order apparent
in it. What, our curiosity compels us to ask, is the mechanism that
causes this Order to come about, not only in our planetary sphere
but on the scale of the stars and galaxies? Today the word {}``cosmology''
has a somewhat narrower interpretation in terms of the study of the
dynamics of the Universe in the language of physics and mathematics,
but though the language might have become more structured the essential
question remains the same.

The first viable mathematical description of a cosmological model
was possible only after the discovery of General Relativity by Albert
Einstein\cite{Einstein:1920RelativityBook}, a framework which allows
us to describe the motion not only of matter but also of the spacetime
manifold in which matter lives. Even though Einstein himself, for
personal and aesthetic reasons, was a proponent of a static universe,
the discovery of solutions for expanding cosmological metrics by DeSitter,
     , Le'Maitre and others pointed elsewhere. The discovery of
the redshifting of the spectra of distant nebulae and galaxies by
Edwin Hubble in 1920\cite{Hubble1929Relation} provided concrete evidence
for an expanding universe. This discovery raised even more vexing
questions. If the Universe is expanding in the present epoch then
at some earlier epoch all the matter and energy must have existed
in the form of an incredibly dense and hot Cosmic egg. This birthing
scenario for the Universe came to be known as the {}``Big Bang''
hypothesis.

At this stage not only gravitational but also quantum
mechanical fluctuations would become significant. Thus a theory of
{}``quantum gravity'' would necessarily be required to properly
describe the earliest epoch of the Universe. Consequently research
in theoretical cosmology since the 1930s until the present has essentially
been a quest for such a unification of the two pillars of modern physics:
Quantum Mechanics and General Relativity.

The lack of a complete understanding of such a unification becomes
stark when considering one of the key elements of General Relativity
as formulated by Einstein - the Cosmological Constant (CC). Einstein
originally introduced this parameter into his field equations in order
to obtain a static cosmological solution which as mentioned above
seemed most natural to him. Including this term the field equations
become \cite{Wald1984General}:

\begin{equation}
G_{\mu\nu}=8\pi G\; T_{\mu\nu}-\Lambda\: g_{\mu\nu}\label{eq:GRFieldEqn}\end{equation}


where $G_{\mu\nu}$ is the Einstein tensor, $T_{\mu\nu}$ is the matter
stress-energy tensor, $g_{\mu\nu}$ is the metric tensor and $\Lambda$
is the CC. Now the simplest form for $T_{\mu\nu}$ is that of a perfect
fluid:

\begin{equation}
T_{\mu\nu}=\rho u_{\mu}u_{\nu}+p(g_{\mu\nu}+u_{\mu}u_{\nu})\label{eq:PerfectFluidTensor}\end{equation}


where $\rho$, $P$ are the density and pressure, respectively, of
the fluid w.r.t an (unaccelerated) observer whose trajectory is given
by the four-vector $u_{\mu}$. With the CC term the effective stress-energy
tensor becomes:

\begin{equation}
T_{\mu\nu}=(\rho+p)u_{\mu}u_{\nu}+\left(p-\frac{\Lambda}{8\pi G} \right)g_{\mu\nu}\end{equation}


or more explicitly for a stationary observer whose trajectory is given
by $u_{\mu}=(-1,0,0,0)$ in a flat background with metric $g_{\mu\nu}=diag(-1,1,1,1)$:

\begin{equation}
T_{\mu\nu}=\left(\begin{array}{cccc}
\rho+\frac{\Lambda}{8\pi G}\\
 & p-\frac{\Lambda}{8\pi G}\\
 &  & p-\frac{\Lambda}{8\pi G}\\
 &  &  & p-\frac{\Lambda}{8\pi G}\end{array}\right)\end{equation}


from which we can see that the CC term behaves as a perfect fluid
with energy density $\frac{\Lambda}{8\pi G}$ and \emph{negative}
pressure $-\frac{\Lambda}{8\pi G}$. This behavior is also characterized
by the \emph{equation of state}:

\begin{equation}
w=\frac{p}{\rho}\end{equation}


In the absence of any other forms of matter we have $\rho=\frac{\Lambda}{8\pi G}$
and $p=-\frac{\Lambda}{8\pi G}$, giving $w=-1$. Thus any form of
matter which has equation of state $w=-1$ has the same physical effect
on cosmological dynamics as the CC term.

For a metric parametrized by a scale factor $a(t)$ and the parameter
$k$, which takes on values $-1,0,1$ corresponding to an open (hyperbolic),
flat and closed ($S^{3}$) spatial topologies respectively:

\begin{equation}
ds^{2}=-dt^{2}+a^{2}(t) \left[ \frac{dr^{2}}{1-kr^{2}}+r^{2} \left( d\theta^{2}+sin^{2}(\theta)d\phi^{2} \right) \right] \end{equation}


and the stress-energy tensor of a perfect fluid with a CC term, only
the diagonal components of the Einstein tensor are non-zero and the
field equations reduce to \cite{Wald1984General}:

\begin{eqnarray}
\frac{\dot{a^{2}}}{a^{2}} & = &\frac{8\pi G\rho}{3}+\frac{\Lambda}{3}-\frac{k}{a^{2}}\label{eqn:Friedmann}\\
\frac{\ddot{a}}{a} & = & -\frac{4\pi G}{3} \left( \rho+3p-\frac{\Lambda}{4\pi G} \right) 
\end{eqnarray}


which are referred to as the first and second Friedmann equations
respectively. We can see from the first equation that for $k=+1$,
there exists a $\Lambda>0$ such that the $H^{2}=\frac{\dot{a^{2}}}{a^{2}}=0$.
For a suitable value of $p$, we can also set the RHS of the 2nd equation
to zero. This solution is known as the Einstein-static universe. It
is clear that without the presence of the CC term in the 2nd term
such a static solution would not be possible. It is an easy exercise
to show that this solution is unstable w.r.t small perturbations in
the scale factor. Therefore the Einstein-static universe, apart from
being in conflict with observation is also not a mathematically stable
solution. Thus Einstein's introduction of the CC term was a failure
in that it cannot be used to obtain a stable static solution. Subsequently
Einstein would refer to the CC term as his {}``greatest blunder''%
\footnote{Even in this {}``blunder'', Einstein had discovered a term that
lies at the crux of the clash between GR and QM and continues until
today to vex theorists and experimentalists alike with its resistance
to a final satisfactory resolution as to its physical origins and
significance.%
}.

As the focus of cosmological research moved on to the study of special
solutions of (\ref{eq:GRFieldEqn}) describing stars and black holes,
the realization of the true significance of this cosmological term
was postponed until the last decade of the 20 century when enough
evidence had accumulated from cosmological observations involving
ground and space-based platforms examples and references HST, COBE etc. to rule out all cosmological models except those with non-zero $\Lambda$ \cite{Spergel2006Wilkinson}.

Though Hubble's discovery provided confirmation that the large scale
structure of the cosmos was amenable to a description within the framework
of Einstein's General Relativity, this was merely the beginning of
the story. It was clear that the observable universe contained a great
deal of structure and that any cosmological model would be considered
incomplete if it did not provide some explanation as to how these
large-scale inhomogeneities could form. A further layer of confusion
was added by Penzias and Wilson's \cite{Penzias1965A-Measurement} discovery of the cosmic
microwave background (CMB) radiation which appeared to be isotropic
and uniform at a temperature of $\sim3^{\circ}$ K, with negligible
fluctuations. This radiation was the imprint of the recombination
era, the time when the universe had cooled enough for electrons and
protons to form hydrogen atoms. Naturally any gravitational inhomogeneities
present at that epoch would have left a signature on the radiation
emitted during recombination. And these same inhomogeneities constituted
the seeds of present day large scale structure. However a simple calculation
assuming a FRW cosmological model, shows that the structures which
lie within our Hubble horizon ($\sim3000\, Mpc\sim10^{26}m$) could
not have been in causal contact at $t_{rec}$. How could one reconcile
this observation with the observed isotropy and homogeneity of the
CMB? We also happen to live in a universe with a predominance of matter
over anti-matter.%
\footnote{The alternative would clearly be unsuitable not only for life but
for any kind of stable structures such as planets and galaxies.%
} Now the Standard Model description of elementary particles does not
incorporate CP violation, which would be required for one form of
matter to be predominant. What mechanism could explain this matter-antimatter
asymmetry?

\section{The Cosmological Constant term in Field Theory}

While the most concrete evidence for a non-zero $\Lambda$ has come
from astronomical observations, one can also investigate this question
from the perspective of local QFT and/or candidates for theories of
Quantum Gravity (QG) such as LQG and String Theory. But first, let
us consider the simple case of the quantum harmonic oscillator.

One of the most striking results of early quantum mechanics was Dirac's
quantization of the harmonic oscillator hamiltonian is the energy
eigenstate basis. The resulting hamiltonian becomes:

\begin{equation}
\hat{H}=\hbar\omega\sum_{\vect{k}}\left(a_{\vect{k}}^{\dagger}a_{\vect{k}}+\frac{1}{2}\right)\end{equation}


where $\omega$ is the characteristic frequency of the given oscillator.
This expression might seem passe to most physicists, having encountered
it one too many times in the literature. However, let's rewrite it
as:

\begin{equation}
\hat{H}=\hat{H}_{kin}+\hat{H}_{\Lambda}=\left(\hbar\omega\sum_{\vect{k}}a_{\vect{k}}^{\dagger}a_{\vect{k}}\right)+\left(\sum_{\vect{k}}\frac{1}{2}\hbar\omega\right)\end{equation}


It is clear that the second term $\hat{H}_{\Lambda}$ is a divergent
sum, at least for a free harmonic oscillator. This is referred to
as the \emph{zero-point energy} and arises due to the commutation
relations of the ladder operators. Its presence clearly indicates
a difficulty. The ground state of a SHO is characterised by a divergent
energy term. In practice, this term is considered a minor annoyance
as in any physical observation it is the energy \emph{difference}
between two states that is measured, and can therefore be ignored
for all practical purposes (FAPP). This is no longer the case when
gravity is included in the picture. These \textquotedbl{}zero-point\textquotedbl{}
fluctuations will now contribute to the expectation value $\expect{T_{\mu\nu}}$
of the stress-energy tensor and hence lead to metric perturbations
which can no longer be neglected. In general, due to the interaction
of matter and geometry as codified by the equivalence principle, any
matter fields will receive some non-zero correction to their self-energy
in a given background metric. In principle this is a good thing. It
should regulate the divergent sums as found above in $\hat{H}_{\Lambda}$
by modifying the dispersion relation at high momenta.

What we refer to as the cosmological constant from the perspective
of QFT is precisely the zero-point contribution to the vacuum energy
from all Standard Model matter fields. This contribution ($\hat{H}_{SM}$)
would constitute the $T_{00}$ term of the vacuum stress-energy tensor
and the solution of the Einstein equations is deSitter spacetime with
$\Lambda_{SM}=\expect{\hat{H}_{SM}}$. $\Lambda_{SM}$ would also
act as a natural momentum cutoff for integrals in the SM.

So far we have not encountered any contradictions. Matter quantum
fields have a divergent zero-point energy. This energy should play
the role of a positive $\Lambda$ when gravity is included in the
picture and should act as a momentum cut-off thereby regulating divergences
in the field theory. Seen this way, gravity is nature's way of keeping
vacuum fluctuations in check \cite{Crane1985Space-time}.

However, the ratio of the CC as obtained from WMAP3 and other cosmological
observations ($\Lambda_{obs}$) happens to be smaller than that calculated
from SM fields ($\Lambda_{SM}$) by a factor of about $10^{-120}$.
What is the explanation for this massive discrepancy? Regardless of
its origin, this number points to a lack of understanding about the SM
and its relation to a possible quantum theory of gravity. In the 1960s, theorists realized
that the mechanism of supersymmetry could alleviate the problem somewhat.
In this picture every SM particle has a supersymmetric dual whose
vacuum fluctuations are of opposite sign as those of the original
particle. This leads to a partial cancellation when computing $\Lambda_{SM}$.
The magnitude of the problem is then reduced but the ratio $\Lambda_{obs}/\Lambda_{SM}$
still remains a gargantuan $10^{-60}$. Also the inclusion of SUSY
raises more questions. Since we don't observe SUSY particles in nature
and none have so far been found in high energy experiments%
\footnote{Though it is believed that the spectrum of high energy cosmic rays
might be due to decaying relic SUSY particles%
}, we need a mechanism to break SUSY at some point before the radiation
era. Also if SUSY is broken then it is hard to see how it could effect
the CC problem in the present epoch. By itself, SUSY is another beautiful
symmetry of nature and will likely be observed at some point either
in the LHC or perhaps in the realm of condensed matter physics.%
\footnote{This is, of course, a very simplified summary of the actual picture.
In QFT $\expect{H_{SM}}$ corresponds to the one-loop energies of
all the possible SM interaction vertices. However, the physical essence
of the reasoning presented above remains the same.%
} However, it does not provide the sought after resolution of the CC problem.

Insert para about how it is $G_{N}^{2}\Lambda_{SM}$ and not $G_{N}$
which determines the strength of the coupling between matter fields
and gravity. As it so happens, in the proper units, $G_{N}^{2}\Lambda_{SM}\sim10^{120}\Lambda_{obs}$.
How does this factor into the picture ???

Of course, one might ask if it even makes sense to compare two quantities
associated with vastly different scales: $\Lambda_{SM}$ corresponding
to the scale of electroweak theory ($\sim10^{-15}m$) and $\Lambda_{obs}$
corresponding to the scale of CMB horizon ($\sim10^{26}m$). Assuming
that the large-scale structure of geometry can be described by a condensate,
such a comparison becomes essentially meaningless. We know from our
knowledge of many-body physics that fluctuations above the correlation
length of a condensate are strongly suppressed relative to microscopic
scales. If we wish to adopt this perspective, then we must explain
how such a condensate can form and how its correlation length comes
to exceed the range of the weak force. One might speculate that given
a reasonably complete picture of condensate formation the second question
might be answered naturally. In order to further such speculations we need to better understand the nature of quantum mechanics and its role in a gravitational setting.

\section{The Path to Quantum Gravity}

Besides General Relativity, the other great epistemological development in theoretical physics in the 20$^{\textrm{th}}$ century was that of the theory of Quantum Mechanics in the decades between the 1$^{\textrm{st}}$ and 2$^{\textrm{nd}}$ World Wars. As opposed to General Relativity and its offspring, Cosmology, which were built around astronomical observations, the origin of Quantum Mechanics was rooted in studies of the atomic structure of matter. Experimental and theoretical discoveries regarding the absorption and emission spectra of monoatomic gases, the molecular nature of matter as manifested in Einstein's theory of diffusion and the behaviour of particles with an inherent \textit{spin} degree of freedom, among other developments, led physicists to the a new paradigm regarding the structure of matter and radiation. Its salient features were the following:

\begin{itemize}
\item The spectrum of all radiation emitted and absorbed by atomic matter is discrete leading us back to a corpuscular picture of light \emph{a la} Newton. This also implies that the bound states of matter have discrete excitation spectra\footnote{This conclusion still holds but has less significance when considering the spectra of large collections of objects such as in solid state physics. There, as is well known, the spectrum comes in energy "bands" within each of which the spectrum can be said to be continuous because of the massive number of very closely spaced levels.}
\item The outcomes of experiments are always probabilistic in nature with probabilities given by the modulus squared($||\Psi||^{2}$) of a \emph{complex} wavefunction $\Psi$
\item A quantum system can be said to be in a \emph{superposition} of some elementary (eigen)states in between observations. The resulting notion of interference between different states has no analog in classical physics.
\end{itemize}

Initially the successes of the new quantum mechanical formulation were most evident in atomic and molecular physics. The relationship between quantum and classical mechanics was formalised in the Correspondence Principle according to which the dynamics of a system must become amenable to a classical description in the limit of small $\hbar$, when quantum effects become negligible and/or in the limit when the system becomes so large that it is no longer feasible to isolate it from its environment, thus exposing it to the phenomenon of decoherence.

Quantum Mechanics presented far more epistemological quandaries for philosophers of science than General Relativity. Even though the concepts of a curved space-time, or indeed of \emph{space-time} itself, and the Equivalence Principle were a vast leap from Newtonian mechanics, among the community of mathematical physicists the foundations for a physical understanding of non-Euclidean geometry had been laid in the early 19$^\textrm{th}$ century by the work of Gauss, Riemann, Lobachevsky and others. This was not the case for quantum theory which represented an even more radical departure from the accepted precepts of Natural Philosophy. It can be said with some justification that the concepts of superposition and entanglement in quantum theory are far more baffling to the human intuition than those of general covariance or geodesic motion on a curved manifold. Perhaps no where is this better exemplified than via two thought experiments. 

The first one, due to Schrodinger, involves placing a cat in a box equipped with a box containing a cyanide capsule, a photodetector, a beam splitter and a single photon source. The box is closed and isolated from the external environment. The photon source is set on a timer to emit precisely one photon after some interval. The beamsplitter is placed between the light source and the photodetector with there being an equal chance for the photon to pass through the beamsplitter to the photodetector or be reflected away. If the photodetector receives a photon it activates a mechanism which drops the cyanide capsule on the floor of the box. Now for an external observer, who only knows at what time the photon was supposed to be emitted, but not whether it passed through the beam-splitter or was reflected away, the photon will be in a superposition of the two states $\ket{photon} = (\ket{transmitted}+\ket{reflected})/\sqrt{2}$. Thus all we know, until we open the box, is that there is a 50\% probability of the cyanide detector having fallen to the floor thus killing the cat and a 50\% probability for the cat to still be alive. Of course there are many possible criticisms of this setup, one of which is the question of the applicability of the quantum theory to a complex, many-body system such as a cat.\footnote{Another is the question of whether the quantum state of a photon would survive amplification to the level required to execute a mechanical action such as dropping the capsule.} As we shall see the central thrust of this thesis is an attempt to answer this question in a cosmological context, albeit in a more refined form.

The second thought-experiment is due to Einstein (\cite{Einstein1935_EPR}) and his collaborators Nathan Rosen and B. Podolsky. The EPR setup confronts the quantum notions of entanglement and measurement-induced state collapse with the relativistic notions of causality and locality. This thought experiment exploits the quantum mechanical property called \emph{entanglement}. Consider two quantum systems with Hilbert spaces $\mathcal{H}_a$ and $\mathcal{H}_b$ respectively. Then the hilbert space describing the composite systems of two photons is given by the tensor product of the two subspaces: $\mathcal{H}_{ab} = \mathcal{H}_a \otimes \mathcal{H}_b$. In this case our elementary system is a photon with two polarization states: $\{\ket{0},\ket{1}\}$. The hilbert space $\mathcal{H}_{ab}$ describing the system of two photons is a four-dimensional space with the basis: $\{ \ket{00}, \ket{01}, \ket{10}, \ket{11} \}$. States in this hilbert space fall into two categories. Those which can be written as a product of two states in the respective subspaces, e.g. $\alpha \ket{00} + \beta \ket{01} = \ket{0}_a \left( \alpha \ket{0}_b + \beta \ket{1}_b \right)$ and those which cannot be factored in this manner, e.g. $\ket{01} + \ket{10}$ or $\ket{00} + \ket{11}$. The latter variety are called \emph{entangled} states because of this property of non-factorizability.

The setup of the EPR experiment involves two spatially separated observers, traditionally referred to as 'Alice' $\bra{Alice} $ and 'Bob' $\bra{Bob}$. Two photons are prepared in an entangled state, say $\ket{\Psi} = \alpha \ket{0_A 1_B} + \beta \ket{1_A 0_B}$ (the subscripts denote which observer a photon is sent to), separated by a beam splitter and sent to Alice and Bob respectively. Now let Alice conduct a measurement on her part of the entangled state by projecting it along $\bra{0_A}$ component in \emph{her}  basis. This yields $\innerp{0_A}{\Psi} = \alpha \ket{1_B}$ showing that if Alice measures her photon to be in the $\ket{0_A}$ state, then she \emph{knows} that Bob's photon is necessarily in the $\ket{1_B}$ state and vice-versa. Since the separation between Alice and Bob is spacelike, this implies that a measurement on an entangled state at one location has an instantaneous, non-local effect on a measurement in a distant location.

While these phenomena of superposition and entanglement are not easily accessible to classical intuition, after a certain amount of effort one can get used to invoking them on a regular basis in solving problems involving atomic systems. After all, though the position of an electron might be an uncertain variable, it does not stretch the imagination too much to imagine the electron not as a point particle, but a diffuse cloud or \emph{wavefunction}. Such a feat is far harder when considering a large, complex, macroscopic many-body system such as a coffee mug or a cat. Clearly some fundamental shift occurs in the character of the physical theories describing a system as its size is scaled up from micro to macro scales.

Gravity in particular is not a theory of point particles\footnote{In the sense that it is only aggregates of atoms which exert any appreciable gravitational force}. The domain in which the gravitational force becomes substantial to be of concern is firmly within the realm of many-body phenomena. This observation could provide a resolution of the \emph{hierarchy problem}\footnote{It is clear that such a hierarchy of fundamental constants is essential for the formation of a complex universe with planets which can support life. It cannot be a coincidence that life emerges in the same range of scales as gravity ($\sim$ 1 m). The theoretical problem here is the lack of a coherent description of the symmetry breaking processes in the early universe that lead to the formation of this hierarchy.} in particle physics - what is the origin of the disparity between the energy scales of the gravitational and electroweak forces, which is of order $\sim$ 10\supersc{20} GeV? The traditional QFT methods of approaching this problem are therefore inherently flawed in that they do not treat gravity as being any different from the other forces.

Of course, one can calculate the gravitational force between two protons just as well as for the force between two planetary bodies. But then the following question presents itself: Why in the first case is this force irrelevant from a physical viewpoint, whereas in the second it defines the dynamics of the system? On the other hand, it is (relatively) easy to prepare an entangled state of two photons or two protons but far more difficult (if not impossible) to prepare an entangled state of larger systems\footnote{Even though recent experiments have extendend this bound to pairs of cesium atoms, there is likely to be some limiting size beyond which entanglement between macroscopic objects becomes unfeasible}

As a side note, we should mention that the weakness of the gravitational attraction between microscopic objects ranging from protons to single-celled bacteria, does not imply the same for all mesoscopic objects. A black hole with a mass equal to that of the Earth would be of the order of the size of a tennis ball. Two such "tennis balls" placed in close vicinity would exert a considerable gravitational force on each other.

These considerations make explicit the deep conceptual conflicts between General Relativity and Quantum Mechanics.

\section{Many-Body Phenomena in Cosmology}

Cosmology is the ultimate area for many-body phenomena. Despite the past and continuing focus on simplified models such as the FLRW or LCDM cosmologies, it is gradually becoming clear that such single-body descriptions of Nature are sorely incomplete. Observations from galaxy surveys such as the SDSS (Sloan Digital Sky Survey), 2dF-GRS (2 degree Field of view - Galaxy Redshift Survey) and others are revealing inhomogeneities on the largest scales. Models of structure formation involving merging of dark matter halos are yielding a picture of a self-similar Cosmic Web (as in \cite{Sheth2003Hierarchy,Sheth2005Exploring}) in which most of the matter is distributed along (quasi) one-dimensional filaments and at nodes where these filaments meet. The space between these structures is filled with voids with a scale-invariant size distribution. The standard tools of homogenous, isotropic cosmologies are woefully inadequate for describing such richness of structure.

In the following we give a few examples of many-body phenomena in classical and quantum mechanics.

\subsection{\dots in the realm of Classical Physics}

The simplest example of many-body phenomena in the Newtonian framework occurs in the three body problem, which has no general analytical solution.

\subsection{\dots in the realm of Quantum Physics}

\section{Gravitation as a Many-Body Phenomenon}

In this thesis we show how a fermionic gas in a time-dependent background
can undergo condensation and the resulting condensate can be used
to generate inflation. Now, while this approach utilizes the notions
of many-body physics in a gravitational background, it does not directly
address the question of whether gravitation itself is best understood
as many-body phenomena. A detailed analysis of this assertion is done
in other work. Here we briefly outline the physical motivations for
making such a claim. As a reminder we briefly review the basic notions of the inflationary paradigm.

\subsection{Inflationary Cosmology}

In 1982 Alan Guth, and Vilenkin and Starobinsky in the following year,
proposed a new paradigm for the early universe: the inflationary model.
The elegance of this solution lay in the fact that it had the potential
to explain all three of the above problems (horizon/flatness problem,
structure formation and matter-antimatter asymmetry) in one stroke.
The idea behind inflation is simple enough. As the word implies, the
universe underwent a period of exponential growth when the scale factor
$a(t)$ grew as $\sim e^{-H_{in}\,t}$ ($H_{in}$ is the Hubble rate
during this era) starting from some small homogenous, isotropic patch
of geometry. This rapid expansion would damp out any large scale geometrical
inhomogeneities present prior to inflation, avoiding the need to fine-tune
the matter density at early epochs, and would naturally yield the
flat, isotropic background as characterized by the CMB, leaving only
small density fluctuations whose amplitude ($\delta\rho/\rho\sim10^{-5}$)
depends only on the duration of the inflationary phase. Being a non-equilibrium
process, inflation provides the conditions necessary for CP violating
processes to occur, which would then naturally yield an excess of
particles over anti-particles.

Despite these attractive features, the biggest obstacle to accepting
the validity of this model was the lack of a sensible physical mechanism.
Guth's original proposal was that of \emph{false vacuum decay}. The
initial state of geometry is described by a quantum state that is
trapped in the false minimum of a potential. As the temperature falls,
the potential is lowered sufficiently to allow the state to tunnel
out resulting in bubbles of inflationary patches to grow. This model
however ran into trouble because the speed of expansion of these bubbles
was insufficient to allow them to combine and reach equlibrium, resulting
in a highly inhomogenous universe at the end of the inflationary phase,
in conflict with the homogeneity of the CMB. The other most commonly
adopted class of models was based on the notion of \textquotedbl{}slow-roll\textquotedbl{}.
A scalar field can be shown to have a negative equation of state,
the same as a positive cosmological constant and can thus drive inflation.
In order to obtain the required duration of inflation, followed by
a period of reheating, the potential for this scalar would in general
have to have a very specific form determined by the so-called \textquotedbl{}slow-roll\textquotedbl{}
conditions. The ad-hoc nature of this potential and the lack of a
suitable scalar field in the Standard Model are the main deficiencies
of this class of models%
\footnote{The SM is posited to contain the U(1) Higgs axion which is supposed
to endow particles with mass via a sponatenous symmetry breaking mechanism.
It is unclear however if the Higgs would correspond to the inflaton
and if so how could they be related.%
}. In recent years, alternatives to inflation such as the ekpyrotic
and bouncing universe scenarios have been proposed. String Theory
has also provided fertile ground for alternatives to inflation such
as the brane collision model \cite{Alexander2002Inflation}. However, none
of these alternatives has the simplicity and elegance of the inflationary
scenario. The question then arises: can we come up with some mechanism
which generates inflation without resorting to ad-hoc potentials or
exotic scalar/quintessence fields which \emph{a priori} don't have
sufficient grounding in our (reasonably complete) picture of the Standard
Model of particles?

In this thesis, we argue that a cosmological condensate which forms
via the BCS mechanism can source inflation without resort to ad-hoc
potentials and also provide a resolution to the cosmological constant
problem. Venturing into the speculative realm, we also conjecture
that such an axion would play the role of the Higgs scalar. Inflation
has the character of a phase transition. Phase transitions in particle
physics are generally understood within the framework of \emph{spontaneous
symmetry breaking} (SSB). An axion with U(1) symmetry could undergo
SSB and be responsible for both driving inflation and endowing leptons
with mass, thus playing the role of the inflaton + Higgs.

\subsection{A Many-Body model of deSitter}

The simplest physical models of inflation rely on the deSitter metric:

\begin{eqnarray}
ds^{2} & = & dt^{2}-a(t)^{2}d\mathbf{r}^{2}\nonumber \\
 & = & \frac{1}{H_{0}^{2}\eta^{2}}(d\eta^{2}-d\mathbf{r}^{2})\end{eqnarray}


where $a^{-1}(\eta)=H_{0}\eta$ {[}check and fix details ...{]} and
$\eta\in[-\infty,0]$. In these models the gravitational background
is taken as a given in terms of some prescribed metric such as in
\ref{eqn:deSitterMetric} or some version thereof, coupled to some
matter field $\phi$\;%
\footnote{which satisfies the negative energy equation of state: $w=-1$%
} with a potential $V(\phi)$. The search for viable models of inflation
has generally involved finding good candidates for the matter fields
and the corresponding potentials. Any such model of inflation will
necessarily be only a poor approximation to a more complete picture
of quantum gravity where matter and gravitational degrees of freedom
are treated in a unified manner.

In these approaches to inflation one fundamental aspect of general
relativity is ignored - that gravity describes a system with precisely
two degrees of freedom at each point of space-time. This is most easily
via the ADM formulation of GR wherein we find that Einstein's equations
can be expressed as a sum of constraints. The Ricci curvature tensor
$R_{\mu\nu}$ in D dimensions has $N=(D-2)(D-1)/2$ (Appendix 2 in \cite{Wald1984General}) degrees of freedom. For D=4, this yields N=6. We have
four constraint equations (one from the scalar and three from the
diffeomorphism constraint) giving us two free degrees of freedom at
each point of a 3+1 dimensional background. In light of this observation
the picture that comes to mind is that of a spin-system such as those
encountered in condensed matter models. If we were to proceed under
the assumption that such an analogy has more than merely formal content,
then one can immediately export the tremendous insights gained from
condensed matter physics to the gravitational arena.

In such a framework the gravitational variables such as the scale
factor $a(t)$ and the corresponding conjugate momenta $\dot{a}(t)$
are best understood as coarse-grained expectation values of local
operators defined on the spin-system, in the same manner as the magnetization
in the Ising model corresponds to the average of the spins at all
sites of the given lattice. The exponential growth of the scale factor
in inflationary scenarios can then be interpreted as corresponding
to the divergence of the correlation lengths of order parameters near
a critical point in a spin-system. This point of view is also in concordance
with the picture emerging from studies of quantum geometry where the
spatial manifold is replaced by discretized structures called \emph{spin
networks} whose edges are labeled by spins and vertices are labeled
by so-called \emph{intertwiners} - which live in the space of linear
operators $\mathcal{I}:\bigotimes\limits _{i\in[1..n]}\mathcal{H}_{j_{i}}\rightarrow{\rm {C}}$
where $\{j_{i}\}$ are the spin labels of the edges incident on that
vertex. If one asked a condensed matter physicist what this picture
reminds them of, the immediate answer would be: the Ising model !.
Or one could ask a lattice QCD expert and they would remark on this
model's similarity to their own work. After all the action for Yang-Mills
theory - which with gauge group $SU(3)$ is used to model the strong
interaction - is:

\begin{equation}
S_{QCD}=\int dtd^{3}x\, Tr \left[ F_{\mu\nu}^{IJ}F^{\mu\nu\, KL}\sigma_{IJ}\sigma_{KL} \right] \end{equation}


where $\sigma_{IJ}$ are the generators of the relevant gauge group
($SU(3)$ for QCD) and $F_{\mu\nu}^{IJ}$ is the curvature of the
gauge field. This action the same essential structure as the action
for gravity. We elaborate with some mathematics:

Following Smolin \cite{Smolin2002Quantum}, we have for the Hamiltonian
constraint for GR with positive $\Lambda$:

\begin{equation}
\mathcal{H}_{deS}=\epsilon_{ijk}E^{ai}\left(F_{ab}^{k}E^{bj}-\frac{\Lambda}{3}\epsilon_{abc}E^{bj}E^{ck}\right)=0\label{eqn:deSitterHamiltonian}\end{equation}


We would like to point the formal similarity between this equation
and the hamiltonian for condensed matter systems, in particular the
spin-ice model, where our degrees of freedom are spins $S_{i}$ placed
at the vertices of a hexagonal lattice ${}^{\star}\mathcal{L}$. The
dual $\mathcal{L}$ of this lattice is a triangular lattice. The spins
can also be seen as being located on the faces of $\mathcal{L}$.
This makes sense from the quantum geometry framework where the area
operator of a surface is the Casimir $J^{2}$ of a system of spins
$j_{i}$ labeling each point on the surface $p_{i}$ which is pierced
by a loop carrying a flux of the gravitational connection. For this
to work however, we must dimensionally reduce \ref{eqn:deSitterHamiltonian}
which \emph{a priori} is the Hamiltonian of a system in three dimensions.
This can be done by considering a foliation of the three dimensional
space with two dimensional sheets. We can fix a gauge in which $E^{0i}=n^{i}$
is the normal to these two dimensional surfaces. Then the first term
in \ref{eqn:deSitterHamiltonian} becomes:

\begin{eqnarray}
\mathcal{H}_{1} & = & \epsilon_{ijk}E^{ai}E^{bj}F_{ab}^{k}\nonumber \\
 & =\end{eqnarray}


The deSitter Hamiltonian $\mathcal{H}_{deS}$ can then be interpreted
as being the sum of the terms corresponding to the kinetic energy
and the nearest neighbor, two and three body interaction energies
of spins $E^{ai}$ placed at the vertices of the hexagonal lattice
(${}^{\star}\mathcal{L}$). The two and three body interaction energies
are:

\begin{equation}
\mathcal{E}_{2}=\sum F_{ij}^{ab}E_{a}^{i}E_{b}^{j};\qquad\mathcal{E}_{3}=-\frac{\Lambda}{3}\sum\epsilon_{ijk}\epsilon^{abc}E_{a}^{i}E_{b}^{j}E_{c}^{k}\label{eqn:nBodyEnergy}\end{equation}


where $i,j,k$ label vertices in $^{\star}\mathcal{L}$ and $a,b,c$
label the possible states of each spin variable. From the form of
the above equations it is clear that the {}``spins'' in this case
have to live in a three-dimensional hilbert space $H_{3}$. The two-body
interaction term contains the kinetic energy term which is given by:

\begin{equation}
E_{kin}=\frac{1}{2}\sum_{i}F_{ii}^{ab}E_{a}^{i}E_{b}^{i}\label{eqn:kinTerm}\end{equation}


The remaining components of the two-body term can be interpreted as
exchange energies:

\begin{eqnarray}
E_{exch} & = & \sum_{i\neq j}F_{ij}^{ab}E_{a}^{i}E_{b}^{j}\label{eqn:exchTerm}\\
 & = & \frac{1}{2}\sum_{i}\sum_{j>i}F_{ij}^{ab}\left(E_{a}^{i}E_{b}^{j}\pm E_{a}^{j}E_{b}^{i}\right)\end{eqnarray}


where the sign in the last term determines the statistics the particles
$E_{a}^{i}$ obey under exchange.

When restricted to a 2D space, in addition to fermionic and bosonic
statistics we can have anyonic statistics, i.e. exchanging two identical
objects can lead to a phase change of $e^{\imath\theta}$. The exchange
term should then be written as:

\[
^{2D}E_{exch}=\frac{1}{2}\sum_{i}\sum_{j>i}F_{ij}^{ab}\left(E_{a}^{i}E_{b}^{j}+e^{\imath\theta}E_{a}^{j}E_{b}^{i}\right)\]

where the anyon phase factor is included.

\section{The Bardeen-Cooper-Schreiffer Theory}



\section{Elements of LQG}

There are two principle approaches to the problem of reconciling gravitational
physics with quantum theory. The first is String Theory (ST) which
is founded around the study of excitations of extended objects - strings
and higher dimensional branes - embedded in a flat spacetime. The
second is Loop Quantum Gravity (LQG) which attacks the problem from
a different perspective, one that seeks to preserve the principle
feature of general relativity - background independence. Here the
notion is that by quantizing around a flat background - as is done
in ST - we sacrifice background independence and then there is no
guarantee that the resulting theory can correctly describe the quantum
fluctuations of geometry especially in the strong-field regime.

The principle obstacle to covariant quantization approaches was the
non-renormalizability of the gravitational action, a problem rendered
even more difficult due to its non-quadratic form. The Einstein-Hilbert
action is:

\begin{equation}
S_{EH}=\int d^{4}x\sqrt{-g}\mathcal{R}\label{eqn:EinsteinHilbert}\end{equation}


where $\mathcal{R}$ is the Ricci scalar and $g$ is the determinant
of the metric element. One can see that due to the non-polynomial
nature of this action ...


\section{Cosmological Condensates}


\section{WMAP - New eyes upon the Cosmos}

As mentioned above, Einstein introduced the $\Lambda$ parameter %
\footnote{For obvious reasons, we feel that the \textquotedbl{}cosmological
constant\textquotedbl{} not truly being a \emph{constant} deserves
a different designation%
} in order to fulfill his aesthetic vision of a static universe. After
observations ruled out a static universe the $\Lambda$ term faded
into obscurity until astronomical observations in the latter half
of the 20th century provided evidence for an accelerating universe.
The most recent and precise of these observations have come from the
Wilkinson Microwave Anisotropy Probe or WMAP for short. The results
from WMAP3%
\footnote{the suffix '3' denoting the latest data run%
} and measurements of supernovae, galaxy redshift surveys and large-scale
structure surveys such as SDSS are all consistent with a $\Lambda$CDM
cosmological model - suggesting a universe dominated by dark energy
in the form of $\Lambda$ and cold dark matter (CDM), with hot baryonic
matter constituting only about $10\%$ of the present day matter density%
\footnote{the recent observations of the ballistic collision of two galaxies
in the Bullet Cluster seems to have answered in the affirmative the
question of the \emph{existence} of dark matter%
}.

The Friedmann equations (\ref{eqn:Friedmann}) contains the following
constants: $G,\Lambda,k$ and the variable quantities are $a(t),\dot{a}(t)$
or their combinations $a(t)/\dot{a}(t)\sim H$. How do we determine
the values of these quantities from experiment and observation? Many
experiments {[}citations{]} have determined the value of $G$ to be
$\sim6.67\times10^{-7}Nm^{2}/kg^{2}$. However most of these have
been earth-based. Recent studies have suggested that one way to look
at the $\Lambda$ problem is by allowing $G$ to vary on cosmological
scales \cite{Dvali2007Degravitation}. This phenomenon is referred to as degravitation. But in order to determine the correct extension of GR which
would incorporate this idea, we have to test the limits of the validity
of simple cosmological model given by (\ref{eqn:Friedmann}) with
constant $G,\Lambda$ and $k$. A viable inflationary model ends up
in a Friedmann universe within a radiation background with some finite,
pseudo-constant values for these parameters, because that is the state
compatible with the CMB. From that point on cosmological evolution
on different scales decouples due to gravitational collapse and thereafter
the large scale evolution of the universe can to some extent be described
independently of the structure-formation processes occurring at smaller
scales.


\section{Beyond the Standard Model}


\subsection{Topological Defects and Vacuum Energy}

By now it is generally accepted that topological considerations will
play a major role in any theory of Quantum Gravity. The course of
development of theoretical physics over the 20$^{\text{th}}$ coincides
with attempts to generalize the goemetrical framework which undergirds
particles and their interactions. First the Special and then the General
Theory of Relativity extended the backdrop for physical phenomena
from Galilean to a Lorentzian and finally to a pseudo-Riemannian manifold.
The realization that geometry is itself dynamical lead to efforts
by Kaluza-Klein, Einstein, Weyl and others to construct a field theory
incorporating gravity and electromagnetism. In fact Weyl was lead
to the first formulation of the gauge principle in theoretical physics
through his attempt at unification.

These early considerations generally did not consider the role that
topology might play in unifying matter and geometry. A notable exception
was the work by Einstein and Rosen \cite{Einstein1935Particle}. Though
most often cited in reference to ``Einstein-Rosen'' bridges (wormholes),
the intent behind the work had nothing to do with wormholes, but with
constructing a singularity free solution of general relativity which
naturally incorporated matter. To cite from the abstract:
\begin{quote}
... These solutions involve the mathematical representation of physical
space by a space of two identical sheets, a particle being represented
by a {}``bridge'' connecting these sheets ... 
\end{quote}
In the absence of concrete physical predictions and a lack of theoretical
interest this model of elementary particles as spacetimes with non-trivial
topology was forgotten. It was revived later in the form of the {}``wormhole''
solution which could conceivably be a model for travel between two
different and vastly separated regions of space (see for eg. \cite{Visser1996Lorentzian})

\subsection{The Braided Universe}

In 2006 Sundance Bilson-Thompson proposed that the particles of the
Standard Model (SM), or at least those in the first generation: the
leptons consisting of the electron, electron-neutrino and the up and
down quarks and the gauge bosons ($W^{\pm}$, $Z_{0}$, $\gamma$)
could be given a unified representation in terms of the irreducible
elements of the first non-trivial braid group ($B_{3}$).%
\footnote{To be precise, he used an enlargement of the braid group. Physically
this consists of replacing the 1D threads of the braid with 2D ribbons
which can then contain twists (or orientation). Mathematically this
is the product group $\tilde{B}_{3}=B_{3}\times Z_{2}$ - i.e. the
product of the simplest abelian and the simplest non-trivial braid
group.%
}

He then showed that the irreducible elements of $\tilde{B}_{3}$ can
be put into one-to-one correspondence with (at least) the first generation
of the SM particles in a very natural manner. Despite the elegance
of the construction - for instance all particles have left and right-handed
representations, except for the neutrino which comes in only one handedness
- some significant physical questions remained unanswered in \cite{BilsonThompson2006Quantum}.
In the following we elaborate on these missing pieces.

LQG and String Theory both remain a few steps away from giving a coherent
description of quantum gravity which naturally incorporates the particles
of the SM - i.e. the so-called goal of \textquotedbl{}Unification\textquotedbl{}.
However, we have obtained a very good notion of what the final picture
should look like from the advances in the respective fields. In fact
now we are faced with a convergence of two supposedly clashing approaches.
Critics of String Theory point to its lack of a natural habitat for
the SM and its many solutions constituting an embarrasment of riches
that is yet to be tamed. However, String Theory is more like a tree
than the idea of one. It doesn't have one indisputable conclusion
or equation, but a plethora of very compelling ideas%
\footnote{Put in examples - such as the Born-Infeld action, String Condensation,
?-Dualities which can prossibly explain the hierarchy problem etc.%
} which, it is safe to say, will emerge naturally in the final analysis.
Likewise the main weakness of LQG (in my opinion), its lack of a particle
spectrum, does not diminish the validity of its physical implications
for quantum geometry.

Given this abundance of theoretical evidence, it is clear that any
notion of particles as topological structures should find a natural
home in LQG and String Theory, for instance the manner in which Ehrenfest's
theorem allows us to make a correspondence between the time evolution
of quantum expectation values and that of classical phase space variables.

Now, at least at a purely visual level, the braid picture seems to
be in concordance with the structures that are natural in both LQG
and ST - Spin-networks whose 1D edges can braid around each other%
\footnote{Indeed, Yidun Wan showed that this process allows us to implement
Bilson-Thompson's picture in LQG - however, not perfectly%
} on the one hand, and 1D strings and higher dimensional brane-like
structures on the other. Unfortunately, this visual similarity begins
and ends at the purely speculative level and can only guide us to
the final answer.

%It has yet to be shown how to correctly embed ribbon-like
%structures in LQG%
%\footnote{It is the author's prejudice that ST and LQG are not descriptions
%of nature at the same scale. Instead ST is in some ways a semi-classical
%cousin of LQG. Thus in the following we will stick with LQG and hope
%to be able to revisit the connection with ST at a later point%
%}. Smolin has shown \cite{Smolin2002Quantum} that in LQG with a positive
%$\Lambda$, for technical reasons, we are required to use framed ribbons
%instead of 1D curves as the edges of our spin-networks. Taking the
%idea further, in \cite{Smolin1995Linking}, he constructed a picture
%which has very strong resemblance to the one we present here. If Bilson-Thompson
%had written his paper 10 years ago, then concievably Lee Smolin might
%have completed the construction long ago. In fact, the author was
%unaware of \cite{Smolin1995Linking} until late into this investigation.
%However the striking parallels, give us greater faith in the validity
%of this construction and also provide a good place for jumping into
%the mathematical details in the following.

\section{Philosophical Implications}

With any new idea comes not just the solution of old problems but also new ways of thinking about the foundations of physics. The notion of a cosmological condensate is an entirely new way of thinking about the origin and evolution of structure in the cosmos and thus is a particularly rich source of new speculations about well-established principles. One such principle which deserve fresh consideration from this perspective is the concept of \emph{inertia}.

If we are to describe the geometry of spacetime in terms of a condensate, then the natural question that arises is the following:

\begin{quote}
\emph{What are the associated quasiparticles of this condensate and what role do they play in the interplay between matter and geometry?}
\end{quote}

It seems natural to assume that if the ground state of the background geometry of the universe is described by a condensate, then the low-energy excitations around this background should have the interpretation of matter. This proposition casts new light on the old question of the inertia of massive bodies and also for our understanding of the equivalence principle.

One of the most basic assumptions at the foundation of modern physics is Newton's First Law of Motion also known as the principle of inertia:

\begin{quote}
\emph{A body in a state of rest or motion stays in that state unless acted upon by an external force.}
\end{quote}

This law can be stated differently in the condensate framework:

\begin{quote}
\emph{In the absence of external forces, an aggregation of quasiparticles undergoes dissipationless flow through the condensate which forms the background geometry.} 
\end{quote}

\section{Conclusion}

The fact that many-body phenomena are ubiquitous in Nature is sometimes obscured by the traditional physicist's practise of understanding systems  by reducing them to the simplest "single-body" form which can then be treated analytically. The power of mathematical methods unleashed after Newton and Leibniz's discovery of the calculus, along with the reductionist mindset prevalent in western science for much of the past 500 years has imbued us with a false sense of confidence in our ability to understand systems in this manner. During the past century a revolution has occurred against this trend, fueled primarily by discoveries in strongly correlated many-body condensed matter systems. Examples of such phenomena are the superfluid and superconducting states for which a complete understanding can be gained only by abandoning this approach built around single-body dynamics and perturbation theory and embracing the idea that \emph{More is Different}\cite{Anderson1972More}. 

\bibliographystyle{plainnat}
\bibliography{../../bib_library}

\end{document}
